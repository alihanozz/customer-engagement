{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n    <img src=\"https://t3.ftcdn.net/jpg/03/32/11/34/360_F_332113482_OTYvz4irs36feOssRkNlIqyDo1oGW53k.jpg\" width=\"20%\" height=\"auto\"/><br>\n    <h1><b><font color=F39823>Data Sciencist</font><font color=5A595E> Case Study</font></b></h1><br>\n    <img src=\"https://t3.ftcdn.net/jpg/03/32/11/34/360_F_332113482_OTYvz4irs36feOssRkNlIqyDo1oGW53k.jpg\" width=\"20%\" height=\"auto\"/><br>\n</center>\n<br>\n\n- **[<font color=#D65076>Generic Class Definitions</font>](#1GCD)**\n    <br>[<font color=#5A595E>1. Model Preperation</font>](#1MP)\n    <br>[<font color=#5A595E>2. Outlier</font>](#1O)\n- **[<font color=#D65076>Data Exploration and Feature Engineering</font>](#2DEFE)**\n    <br>[<font color=#5A595E>1. Users' Info</font>](#2UI)\n    <br>[<font color=#5A595E>2. Transaction Data</font>](#2TD)\n    <br>[<font color=#5A595E>3. Notification Data</font>](#2ND)\n    <br>[<font color=#5A595E>4. Merge Data</font>](#2MD)\n- **[<font color=#D65076>Model</font>](#3M)**\n    <br>[<font color=#5A595E>1. Customer LifeTime Value (CLV)</font>](#3CLV)\n    <br>[<font color=#5A595E>2. LightGBM</font>](#3L)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T13:57:41.095642Z","iopub.execute_input":"2021-08-29T13:57:41.096065Z","iopub.status.idle":"2021-08-29T13:57:41.100313Z","shell.execute_reply.started":"2021-08-29T13:57:41.096033Z","shell.execute_reply":"2021-08-29T13:57:41.099303Z"}}},{"cell_type":"markdown","source":"***\n## <a id=\"1GCD\"><font color=#D65076>1- Generic Class Definitions</font></a><br>\n***\n\n<p>In this phase, there are 2 classes are defined for general data analysis and outlier detection.</p>\n\nThe first class (model_preperation) is consisted of following features:\n\n* **module checker (check_modules):** ensures if necessary modules are installed in the environment.\n* **library importer (import_libraries) :** imports required libraries.\n* **file reader (read_files):** reads or reloads all csv, excel and pickle files given path.\n* **dataset describers (col_stats, df_cols):** describes dataset's information.\n* **column summarizers (cat_sum, num_sum, col_sum):** gives specific informations about columns.\n\nThe second class (outliers) is consisted of following features:\n\n* **check_outliers:** checks if there are outlier in numerical features.\n* **grab_outliers:** returns dataframe with outlier values.\n* **replace_with_thresholds:** returns outlier handled dataframe.","metadata":{"execution":{"iopub.status.busy":"2021-08-29T13:56:48.373012Z","iopub.execute_input":"2021-08-29T13:56:48.373474Z","iopub.status.idle":"2021-08-29T13:56:48.378307Z","shell.execute_reply.started":"2021-08-29T13:56:48.373433Z","shell.execute_reply":"2021-08-29T13:56:48.377006Z"}}},{"cell_type":"markdown","source":"#### &#9654;<a id=\"1MP\"><font color=#009B77> Model Preperation</font></a>","metadata":{}},{"cell_type":"code","source":"class model_preperation:\n    def __init__(self, display_=True, plot_=True, data='/kaggle/input', sep_=','):\n        self.check_modules()\n        self.import_libraries()\n        self.df_list = []\n        self.data = None\n        self.read_files(data, sep_)\n        self.display_ = display_\n        self.plot_ = plot_\n\n    def check_modules(self, modules={'pandas','numpy','matplotlib','seaborn','sklearn'}):\n        print('Checking modules...')\n        self.global_imports('sys')\n        self.global_imports('subprocess')\n        self.global_imports('pkg_resources')\n\n        installed = {pkg.key for pkg in pkg_resources.working_set}\n        missing = modules - installed\n\n        if missing:\n            self.install_modules(missing)\n\n    def install_modules(self, modules):\n        print('installing modules:'+' '.join(modules))\n        python = sys.executable\n        subprocess.check_call([python, '-m', 'pip', 'install', *modules], stdout=subprocess.DEVNULL)\n\n    def global_imports(self, modulename, shortname = None):\n        if shortname is None: \n            shortname = modulename\n\n        print(\"\\t\",\"Import:\",modulename,\"as\",shortname)\n        if '.' not in modulename:\n            globals()[shortname] = __import__(modulename)\n        else:\n            *modulename1, modulename2 = modulename.split('.')\n            modulename1 = \".\".join(modulename1)\n            self.global_imports(modulename1)\n            globals()[shortname] = eval(modulename1 + \".\" + modulename2)\n\n    def import_libraries(self):\n        print('Importing libraries...')\n        \n        self.global_imports('numpy','np')\n        self.global_imports('pandas','pd')\n        self.global_imports('os')\n        self.global_imports('seaborn','sns')\n        sns.axes_style(\"whitegrid\")\n        self.global_imports('matplotlib.pyplot','plt')\n        self.global_imports('datetime.datetime','datetime')\n        self.global_imports('functools.reduce','reduce')\n        self.global_imports('warnings')\n        warnings.simplefilter(action='ignore', category=FutureWarning)\n        pd.set_option('display.max_columns', None)\n        pd.set_option('display.max_rows', 50)\n        pd.set_option('max_colwidth', 100)\n\n    def read_files(self, data, sep_):\n        print('Reading files...')\n        if type(data)!=str:\n            self.data = data\n            exec(f\"self.df_list.append('data')\")\n        else:\n            ext_dict = {'csv':'csv','xlsx':'excel','pickle':'pickle'}\n            for dir_name, _, file_names in os.walk(data):\n                for file_name in file_names:\n                    file = file_name.split('.')\n                    file_ext = file[-1]\n                    if  file_ext in ext_dict.keys():\n                        if file_ext == 'xlsx':\n                            self.check_modules({'openpyxl','xlrd'})\n                        sep_str = f\", sep='{sep_}'\" if sep_!=',' else ''\n                        command = f\"self.{file[0]} = pd.read_{ext_dict[file_ext]}('{os.path.join(dir_name, file_name)}'\"+sep_str+f\")\"\n                        exec(command)\n                        if file_ext != 'pickle':\n                            exec(f\"self.{file[0]} = self.reduce_mem_usage(self.{file[0]})\")\n                        exec(f\"self.df_list.append('{file[0]}')\")\n                        print(f\"\\t {file[0]} is created.\")\n                        print()\n\n    def save_dfs(self, path='/kaggle/output'):\n        for d in self.df_list:\n            full_path = os.path.join(path, d+'.pickle')\n            exec(f\"self.{d}.to_pickle('{full_path}')\")\n            print(f\"{d} saved to {full_path}\")\n\n    def rename_dfs(self, rename_dict={}):\n        self.global_imports('gc')\n        for old, new in rename_dict.items():\n            exec(f\"self.{new} = self.{old}.copy()\")\n            exec(f\"del self.{old}\")\n            exec(\"gc.collect()\")\n            exec(f\"self.df_list.append('{new}')\")\n            exec(f\"self.df_list.remove('{old}')\")\n\n    def set_dataframe(self, dataframe):\n        if len(dataframe) == 0:\n            dataframe = self.data.copy(deep=False) if self.data is not None else dataframe\n        return dataframe\n\n    def dtypes(self, dataframe=[], display_=None):\n        dataframe = self.set_dataframe(dataframe)\n        dtypes_df = pd.DataFrame(dataframe.dtypes).reset_index()\n        dtypes_df.columns = ['COLUMN','DTYPE']\n        dtypes_df['COLS'] = dtypes_df.astype(str).groupby(['DTYPE'])['COLUMN'].transform(lambda x: ', '.join(x))\n        dtypes_df = dtypes_df[['DTYPE','COLS']].reset_index(drop=True).set_index('DTYPE').drop_duplicates()\n        \n        display_ = self.display_ if display_ is None else display_\n        if display_:\n            display(dtypes_df)\n            \n        return dtypes_df\n        \n    def head_tail(self, dataframe=[], count=3, display_=None):\n        dataframe = self.set_dataframe(dataframe)\n        head_tail_df = pd.concat([dataframe.head(count),dataframe.tail(count)])\n        \n        display_ = self.display_ if display_ is None else display_\n        if display_:\n            display(head_tail_df)\n            \n        return head_tail_df\n    \n    def drop_unnecessary_cols(self, dataframe=[], cols=[]):\n        col_stats = self.col_stats(dataframe, display_=False);\n        cols_to_drop_lst = col_stats.NUNIQUE[col_stats.NUNIQUE<=1].keys().to_list()\n        return dataframe.drop(cols_to_drop_lst+cols, axis=1)\n\n    def col_stats(self, dataframe=[], display_=None):\n        dataframe = self.set_dataframe(dataframe)\n        data_frames = [pd.DataFrame(dataframe.dtypes,columns=['DTYPES']),\n                       pd.DataFrame(dataframe.isnull().sum(),columns=['IS_NULL']),\n                       pd.DataFrame(dataframe.nunique(),columns=['NUNIQUE']),\n                       dataframe.quantile([0, 0.01, 0.05, 0.50, 0.95, 0.99, 1]).T]\n        col_stats_df = reduce(lambda left,right: pd.merge(left,right,left_index=True,right_index=True,how='outer'), data_frames)\n        \n        display_ = self.display_ if display_ is None else display_\n        if display_:\n            display(col_stats_df)\n            \n        return col_stats_df\n\n    def df_cols(self, dataframe=[], cat_th=10, car_th=20, id_th=100, display_=None):\n        dataframe = self.set_dataframe(dataframe)\n\n        numeric_cols   = dataframe._get_numeric_data().columns\n        \n        nunique        = dataframe.nunique()\n        df_unique_pct  = nunique/dataframe.shape[0]\n        self.id_cols   = list(set(list(df_unique_pct[(df_unique_pct>0.5)].keys()) + [col for col in list(nunique[nunique>id_th].keys()) if col[-2:].lower() == 'id']))\n        \n        self.cat_cols  = list(dataframe.select_dtypes([\"category\",\"object\"]).columns)\n        self.cat_but_car_cols = [col for col in self.cat_cols if (dataframe[col].nunique() > car_th) and (col not in self.id_cols)]\n        self.num_but_cat_cols = [col for col in numeric_cols  if (dataframe[col].nunique() < cat_th) and (col not in self.id_cols)]\n        self.num_but_cat_but_car_cols = [col for col in self.num_but_cat_cols if (dataframe[col].nunique() > car_th) and (col not in self.id_cols)]\n        self.num_but_cat_cols = [col for col in self.num_but_cat_cols  if col not in self.num_but_cat_but_car_cols]\n        \n        self.cat_cols  = [col for col in self.cat_cols if col not in self.cat_but_car_cols+self.id_cols]\n        self.num_cols  = [col for col in numeric_cols  if col not in self.num_but_cat_cols+self.num_but_cat_but_car_cols+self.id_cols]\n\n        self.date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]        \n        self.id_cols   = [col for col in self.id_cols if col not in self.date_cols]\n        \n\n        stats = pd.DataFrame(index=['ID Cols', 'Date Cols', 'Numeric Cols', 'Categoric (Low Cardinality) Cols', \n                                    'Categoric (High Cardinality) Cols', 'Numeric But Categoric (Low Cardinality) Cols',  \n                                    'Numeric But Categoric (High Cardinality) Cols'])\n        stats['Count'] = [len(self.id_cols), len(self.date_cols), len(self.num_cols), len(self.cat_cols), \\\n                          len(self.cat_but_car_cols), len(self.num_but_cat_cols), len(self.num_but_cat_but_car_cols)]\n        stats['Columns'] = [\", \".join(self.id_cols), \", \".join(self.date_cols), \", \".join(self.num_cols), \", \".join(self.cat_cols), \\\n                            \", \".join(self.cat_but_car_cols), \", \".join(self.num_but_cat_cols), \", \".join(self.num_but_cat_but_car_cols)]\n        \n        display_ = self.display_ if display_ is None else display_\n        if display_:\n            display(stats)\n        \n        return self.id_cols, self.date_cols, self.num_cols, self.cat_cols, self.cat_but_car_cols, self.num_but_cat_cols, self.num_but_cat_but_car_cols\n\n    def plot_cols(self, dataframe=[], col_name=None, kind='bar', palette='Set2'):\n        dataframe = self.set_dataframe(dataframe)\n        plt.figure(figsize=(8,5))\n        \n        if kind == 'hist':\n            sns.set_palette(sns.color_palette(palette))\n            ax = sns.histplot(dataframe[col_name], bins=20, kde=True)\n            plt.xlabel('')\n        if kind == 'bar':\n            sns.set_palette(sns.color_palette(palette))\n            dataframe.index.names = ['index']\n            ax = sns.barplot(x='index', y=col_name, data=dataframe.reset_index())\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n            plt.xlabel('')\n\n        plt.title(col_name)\n        plt.tight_layout()\n        plt.show()\n\n    def cat_summ(self, dataframe=[], col_name=None, threshold=False, display_=None, plot_=None):\n        dataframe = self.set_dataframe(dataframe)\n        df = pd.DataFrame({col_name: dataframe[col_name].value_counts(),\"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)})\n        \n        if threshold!=False:\n            threshold_column = col_name if threshold > 1 else 'Ratio'\n            if threshold <=1:\n                  df[threshold_column] = df[threshold_column]/100 \n            main_df = df[df[threshold_column]>threshold]\n            th_df = pd.DataFrame(df[df[threshold_column]<threshold].sum()).T\n            th_df.index = ['Others']\n            cat_summary_df = pd.concat([main_df,th_df])\n        else:\n            cat_summary_df = df\n        \n        plot_ = self.plot_ if plot_ is None else plot_\n        if plot_:\n            self.plot_cols(cat_summary_df,col_name)\n        \n        display_ = self.display_ if display_ is None else display_\n        if display_:\n            display(cat_summary_df)\n            \n        return cat_summary_df\n\n    def num_summ(self, dataframe=[], col_name=None, display_=None, plot_=None,\n                   quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]):\n\n        dataframe = self.set_dataframe(dataframe)\n        num_summary_df = pd.DataFrame(dataframe[col_name].describe(quantiles).T)\n        \n        plot_ = self.plot_ if plot_ is None else plot_\n        if plot_:\n            self.plot_cols(dataframe,col_name,'hist',\"rainbow_r\")\n            \n        if display_:\n            display(num_summary_df)\n            \n        return num_summary_df\n        \n    def col_summ(self, dataframe=[], target=None, col_name=None, display_=None, plot_=None):\n        dataframe = self.set_dataframe(dataframe)\n        cat_cols, num_cols, cat_but_car_cols, num_but_cat_cols, date_cols = self.df_cols(dataframe)\n        \n        if col_name in cat_cols+num_but_cat_cols:\n            col_target_summary_df = pd.DataFrame({col_name: dataframe.groupby(col_name)[target].mean()})\n        elif col_name in num_cols+cat_but_car_cols:\n            col_target_summary_df = dataframe.groupby(target).agg({col_name: \"mean\"}).sort_values(by=col_name,ascending=False)\n        \n        plot_ = self.plot_ if plot_ is None else plot_\n        if plot_:\n            self.plot_cols(col_target_summary_df,col_name)\n            \n        display_ = self.display_ if display_ is None else display_\n        if display_:\n            display(col_target_summary_df)\n            \n        return col_target_summary_df\n    \n    def reduce_mem_usage(self, df):\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('\\t Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n        for col in df.columns:\n            col_type = df[col].dtype\n            if col_type == object:\n                if df[col].nunique()<300:\n                    df[col] = df[col].astype('category')\n                    continue\n            if str(col_type)[:3] in ['int','flo']:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)\n        \n        end_mem = df.memory_usage().sum() / 1024**2\n        print('\\t Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('\\t Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n            \n        return df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-31T01:16:52.536541Z","iopub.execute_input":"2021-08-31T01:16:52.537073Z","iopub.status.idle":"2021-08-31T01:16:52.689317Z","shell.execute_reply.started":"2021-08-31T01:16:52.537013Z","shell.execute_reply":"2021-08-31T01:16:52.688333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### &#9654;<a id=\"1O\"><font color=#009B77> Outlier</font></a>","metadata":{}},{"cell_type":"code","source":"class Outlier:\n    \n    def __init__(self, dataframe, column, low_quantile=0.05, up_quantile=0.95):\n        self.dataframe = dataframe\n        self.column = column\n        self.low_quantile = low_quantile\n        self.up_quantile = up_quantile\n        self.calculate_data_will_be_affected()\n        \n  \n    def calculate_data_will_be_affected(self):\n        if self.check_outlier():\n            ratio_for_up_limit = (len(self.dataframe[self.dataframe[self.column] > self.dataframe[self.column].\n                                      quantile(self.up_quantile)]) /\n                                  self.dataframe[self.column].shape[0]) * 100\n            ratio_for_low_limit = (len(self.dataframe[self.dataframe[self.column] < self.dataframe[self.column].\n                                   quantile(self.low_quantile)]) /\n                                   self.dataframe[self.column].shape[0]) * 100\n            ratio_of_affected_data = round(ratio_for_up_limit + ratio_for_low_limit, 2)\n            print(\"{} - When we reassign operation in the ({} - {}) range, %{} of data \"\n                  \"will be affected in this operation.\".format(self.column,\n                                                               self.low_quantile,\n                                                               self.up_quantile,\n                                                               ratio_of_affected_data))\n            return ratio_for_up_limit, ratio_for_low_limit, ratio_of_affected_data\n        else:\n            print(\"{} - When we reassign operation in the ({} - {}) range, no data \"\n                  \"will be affected in this operation.\".format(self.column,\n                                                               self.low_quantile,\n                                                               self.up_quantile))\n    \n    def outlier_thresholds(self):\n        quartile1 = self.dataframe[self.column].quantile(self.low_quantile)\n        quartile3 = self.dataframe[self.column].quantile(self.up_quantile)\n        interquantile_range = quartile3 - quartile1\n        up_limit = quartile3 + 1.5 * interquantile_range\n        low_limit = quartile1 - 1.5 * interquantile_range\n        return low_limit, up_limit\n    \n    def grab_outliers(self, index=False):\n        low, up = self.outlier_thresholds()\n        if self.dataframe[((self.dataframe[self.column] < low) |\n                      (self.dataframe[self.column] > up))].shape[0] > 10:\n            return self.dataframe[((self.dataframe[self.column] < low) |\n                             (self.dataframe[self.column] > up))]\n        else:\n            return self.dataframe[((self.dataframe[self.column] < low) |\n                             (self.dataframe[self.column] > up))]\n\n        if index:\n            outlier_index = self.dataframe[((self.dataframe[self.column] < low) |\n                                       (self.dataframe[self.column] > up))].index\n            return outlier_index\n    \n    def check_outlier(self):\n        low_limit, up_limit = self.outlier_thresholds()\n        if self.dataframe[(self.dataframe[self.column] > up_limit) |\n                     (self.dataframe[self.column] < low_limit)].any(axis=None):\n            return True\n        else:\n            return False\n    \n    def replace_with_thresholds(self):\n        low_limit, up_limit = self.outlier_thresholds()\n        if low_limit > 0:\n            self.dataframe.loc[(self.dataframe[self.column] < low_limit), self.column] = low_limit\n            self.dataframe.loc[(self.dataframe[self.column] > up_limit), self.column] = up_limit\n        else:\n            self.dataframe.loc[(self.dataframe[self.column] > up_limit), self.column] = up_limit\n        return self.dataframe","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:16:52.690802Z","iopub.execute_input":"2021-08-31T01:16:52.69114Z","iopub.status.idle":"2021-08-31T01:16:52.710822Z","shell.execute_reply.started":"2021-08-31T01:16:52.691108Z","shell.execute_reply":"2021-08-31T01:16:52.709784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=#009B77>Initial Call for the Model Preperation Class</font>\n\n<b>Note:</b> This field should be run if the pickle files have not been created before. Otherwise [next cell](#next_cell) can be run to save some time.","metadata":{}},{"cell_type":"code","source":"files_path='../input/case-study/Data_Science_case_study/data/'\nmp = model_preperation(data=files_path, plot_=True, display_=True, sep_='|')\nmp.rename_dfs({'notifications_dataset':'ntf_data','transactions_dataset':'trx_data','users_dataset':'user_data'})\nmp.save_dfs('./')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:16:52.71243Z","iopub.execute_input":"2021-08-31T01:16:52.712698Z","iopub.status.idle":"2021-08-31T01:17:11.306971Z","shell.execute_reply.started":"2021-08-31T01:16:52.712672Z","shell.execute_reply":"2021-08-31T01:17:11.3057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='next_cell'><font color=#009B77>Call for the Model Preperation Class</font></a>\n\n<p><b>Note:</b> The cell below should be run after the pickle files are created. (can be run repeatedely)</p>","metadata":{}},{"cell_type":"code","source":"mp = model_preperation(data='./', plot_=False, display_=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:11.30926Z","iopub.execute_input":"2021-08-31T01:17:11.309654Z","iopub.status.idle":"2021-08-31T01:17:12.132832Z","shell.execute_reply.started":"2021-08-31T01:17:11.309612Z","shell.execute_reply":"2021-08-31T01:17:12.131882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## <a id=\"2DEFE\"><font color=#D65076>2- Data Exploration and Feature Engineering</font></a><br>\n***","metadata":{}},{"cell_type":"markdown","source":"#### &#9654;<a id=\"2UI\"><font color=#009B77> Users' Info</font></a>","metadata":{}},{"cell_type":"code","source":"mp.head_tail(mp.user_data);\ncol_stats = mp.col_stats(mp.user_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:12.134228Z","iopub.execute_input":"2021-08-31T01:17:12.134566Z","iopub.status.idle":"2021-08-31T01:17:12.184971Z","shell.execute_reply.started":"2021-08-31T01:17:12.134536Z","shell.execute_reply":"2021-08-31T01:17:12.184211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's cast Activation_Date and Crypto_Curr_Flag\nmp.user_data.Activation_Date  = pd.to_datetime(mp.user_data.Activation_Date)\nmp.user_data.Crypto_Curr_Flag = mp.user_data.Crypto_Curr_Flag.astype(str)\n\n# Create Tenure column\nmp.trx_data.Transaction_Date = pd.to_datetime(mp.trx_data.Transaction_Date)\nnow = mp.trx_data.Transaction_Date.max()\nmp.user_data.loc[:,'_Tenure_in_Days'] = mp.user_data.Activation_Date.apply(lambda x:(now-x).days).astype(np.int16)\n\n# Check corr of notification flags\nacc = np.sum(np.equal(mp.user_data.Email_Notif_Allowed, mp.user_data.Push_Notif_Allowed ))/len(mp.user_data.Email_Notif_Allowed)\nprint(f'Correlation of email and push notification approval is {100*acc:.2f}%')\n\n# Create Notification Column\nmp.user_data.loc[:,'_Notif_Allowed'] = (mp.user_data.Email_Notif_Allowed.astype(str) + mp.user_data.Push_Notif_Allowed.astype(str))\nmp.user_data.loc[:,'_Notif_Allowed'] = mp.user_data.loc[:,'_Notif_Allowed'].map({'00':'0', '01':'1', '10':'1', '11':'1', 'UnknownUnknown':'Unknown'}).astype('category')\n\n# Drop unnecessary columns\nmp.user_data = mp.drop_unnecessary_cols(mp.user_data, ['Activation_Date','Email_Notif_Allowed','Push_Notif_Allowed'])\n\ncol_stats = mp.col_stats(mp.user_data)\nid_cols, date_cols, num_cols, cat_cols, cat_but_car_cols, num_but_cat_cols, num_but_cat_but_car_cols = mp.df_cols(mp.user_data, display_=True, cat_th=32, car_th=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:12.188023Z","iopub.execute_input":"2021-08-31T01:17:12.18849Z","iopub.status.idle":"2021-08-31T01:17:13.765944Z","shell.execute_reply.started":"2021-08-31T01:17:12.18845Z","shell.execute_reply":"2021-08-31T01:17:13.764871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there are outliers for num_cols.\nfor col in num_cols:\n    outlier = Outlier(mp.user_data, col, 0.05, 0.95)\n    display(outlier.grab_outliers()[col].unique())\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:13.767791Z","iopub.execute_input":"2021-08-31T01:17:13.768122Z","iopub.status.idle":"2021-08-31T01:17:13.819494Z","shell.execute_reply.started":"2021-08-31T01:17:13.768079Z","shell.execute_reply":"2021-08-31T01:17:13.818584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all columns' summary\nfor cat_col in cat_but_car_cols:\n    mp.cat_summ(mp.user_data, cat_col, threshold=0.01);\nfor cat_col in cat_cols:\n    mp.cat_summ(mp.user_data, cat_col, threshold=False);\n# Analyze num_cols\nfor num_col in num_cols:\n    mp.num_summ(mp.user_data, num_col)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:13.820621Z","iopub.execute_input":"2021-08-31T01:17:13.820878Z","iopub.status.idle":"2021-08-31T01:17:13.867407Z","shell.execute_reply.started":"2021-08-31T01:17:13.820852Z","shell.execute_reply":"2021-08-31T01:17:13.866639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### &#9654;<a id=\"2TD\"><font color=#009B77> Transaction Data</font></a>","metadata":{}},{"cell_type":"code","source":"mp.head_tail(mp.trx_data);\ncol_stats = mp.col_stats(mp.trx_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:13.868627Z","iopub.execute_input":"2021-08-31T01:17:13.869195Z","iopub.status.idle":"2021-08-31T01:17:16.040982Z","shell.execute_reply.started":"2021-08-31T01:17:13.869151Z","shell.execute_reply":"2021-08-31T01:17:16.040228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's cast Year_Month\nmp.trx_data.Year_Month = mp.trx_data.Year_Month.astype(str)\n\n# Fill null values with \"UNKNOWN\"\nfor col in col_stats[col_stats.IS_NULL>0].index:\n    try:\n        mp.trx_data.loc[:,col] = mp.trx_data[col].fillna('UNKNOWN')    \n    except:\n        mp.trx_data.loc[:,col] = mp.trx_data[col].cat.add_categories(\"UNKNOWN\").fillna('UNKNOWN')\n\ncol_stats = mp.col_stats(mp.trx_data)\nid_cols, date_cols, num_cols, cat_cols, cat_but_car_cols, num_but_cat_cols, num_but_cat_but_car_cols = mp.df_cols(mp.trx_data, display_=True, cat_th=32, car_th=10, id_th=1000)\n# DataFrame[ColName+\"_Imputed\"] =   np.where(DataFrame[ColName].isnull(),1,0)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:16.042256Z","iopub.execute_input":"2021-08-31T01:17:16.042797Z","iopub.status.idle":"2021-08-31T01:17:25.91241Z","shell.execute_reply.started":"2021-08-31T01:17:16.042756Z","shell.execute_reply":"2021-08-31T01:17:25.911106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there are outliers for num_cols.\nfor col in num_cols:\n    outlier = Outlier(mp.trx_data, col, 0.05, 0.95)\n    display(outlier.grab_outliers()[col].max())\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:25.913959Z","iopub.execute_input":"2021-08-31T01:17:25.914391Z","iopub.status.idle":"2021-08-31T01:17:26.332394Z","shell.execute_reply.started":"2021-08-31T01:17:25.914347Z","shell.execute_reply":"2021-08-31T01:17:26.331404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all columns' summary\nfor cat_col in cat_but_car_cols:\n    mp.cat_summ(mp.trx_data, cat_col, threshold=0.01);\nfor cat_col in cat_cols:\n    mp.cat_summ(mp.trx_data, cat_col, threshold=False);\n# Analyze num_cols\nfor num_col in num_cols:\n    mp.num_summ(mp.trx_data, num_col)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:26.333861Z","iopub.execute_input":"2021-08-31T01:17:26.334268Z","iopub.status.idle":"2021-08-31T01:17:29.23499Z","shell.execute_reply.started":"2021-08-31T01:17:26.334221Z","shell.execute_reply":"2021-08-31T01:17:29.234246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### &#9654;<a id=\"2ND\"><font color=#009B77> Notification Data</font></a>","metadata":{}},{"cell_type":"code","source":"mp.head_tail(mp.ntf_data);\ncol_stats = mp.col_stats(mp.ntf_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:29.237077Z","iopub.execute_input":"2021-08-31T01:17:29.237493Z","iopub.status.idle":"2021-08-31T01:17:29.328572Z","shell.execute_reply.started":"2021-08-31T01:17:29.23745Z","shell.execute_reply":"2021-08-31T01:17:29.327529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's cast Process_Date\nmp.ntf_data.Process_Date = pd.to_datetime(mp.ntf_data.Process_Date)\n\n# Drop unnecessary columns\nmp.ntf_data = mp.drop_unnecessary_cols(mp.ntf_data, ['Unnamed: 0'])\n\ncol_stats = mp.col_stats(mp.ntf_data)\nid_cols, date_cols, num_cols, cat_cols, cat_but_car_cols, num_but_cat_cols, num_but_cat_but_car_cols = mp.df_cols(mp.ntf_data, display_=True, cat_th=32, car_th=10, id_th=1000)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:29.331614Z","iopub.execute_input":"2021-08-31T01:17:29.331871Z","iopub.status.idle":"2021-08-31T01:17:29.474585Z","shell.execute_reply.started":"2021-08-31T01:17:29.331846Z","shell.execute_reply":"2021-08-31T01:17:29.473767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all columns' summary\nfor cat_col in cat_but_car_cols:\n    mp.cat_summ(mp.ntf_data, cat_col, threshold=0.01);\nfor cat_col in cat_cols:\n    mp.cat_summ(mp.ntf_data, cat_col, threshold=False);\n# Analyze num_cols\nfor num_col in num_cols:\n    mp.num_summ(mp.ntf_data, num_col)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:29.476161Z","iopub.execute_input":"2021-08-31T01:17:29.476429Z","iopub.status.idle":"2021-08-31T01:17:29.500149Z","shell.execute_reply.started":"2021-08-31T01:17:29.476403Z","shell.execute_reply":"2021-08-31T01:17:29.499477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"channel_comm_reason    = mp.ntf_data.groupby(['Communication_Reason','Channel'])['Customer_ID'].count()\nchannel_comm_reason_df = pd.DataFrame(channel_comm_reason)\nchannel_comm_reason_df = channel_comm_reason_df[channel_comm_reason_df['Customer_ID']!=0].reset_index()\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\nfig.tight_layout()\nax = sns.barplot(x=\"Communication_Reason\", hue=\"Channel\", y=\"Customer_ID\", data=channel_comm_reason_df, palette='turbo', ax=axes[0]);\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nsns.heatmap(channel_comm_reason.unstack(), cmap='GnBu', square=True, robust=True, ax=axes[1]);","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:29.501043Z","iopub.execute_input":"2021-08-31T01:17:29.501476Z","iopub.status.idle":"2021-08-31T01:17:31.015077Z","shell.execute_reply.started":"2021-08-31T01:17:29.501427Z","shell.execute_reply":"2021-08-31T01:17:31.014397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### &#9654;<a id=\"2MD\"><font color=#009B77> Merge Data</font></a>","metadata":{}},{"cell_type":"markdown","source":"<p>Next mission is merging all datasets by <b>Customer_ID</b>. Before moving on to this, <u>some aggregations/calculations have to be made.</u></p>\n\n<p><b>For the notification dataset:</b> <font color=#DD4124>Notification Data Channel</font> and <font color=#DD4124>Communication Reasons</font>' count can be pivoted per customer as follows:</p>\n    \nNote that, the aggregations are done in 2 phases, including all data and 2021 only. ","metadata":{}},{"cell_type":"code","source":"## Stacking process\nstacked = mp.ntf_data.groupby(['Customer_ID','Communication_Reason','Channel'])[['Customer_ID']].count()\nstacked.columns = ['Sum']\nstacked = stacked.reset_index().pivot(index='Customer_ID',columns=['Communication_Reason','Channel'], values='Sum')\nstacked.columns = ['_'+x for x in stacked.columns.map('_'.join)]\n\n## Same process for only 2021 data\nstacked2021 = mp.ntf_data[mp.ntf_data.Process_Date.dt.year == 2021].groupby(['Customer_ID','Communication_Reason','Channel'])[['Customer_ID']].count()\nstacked2021.columns = ['Sum']\nstacked2021 = stacked2021.reset_index().pivot(index='Customer_ID',columns=['Communication_Reason','Channel'], values='Sum')\nstacked2021.columns = ['_'+x+'_2021' for x in stacked2021.columns.map('_'.join)]\n\n## Drop unnecessary columns\ndrop_list = []\nfor col in stacked2021.columns:\n    if sum(stacked2021[col].value_counts()/stacked2021.shape[0]>0.05)<2:\n        drop_list.append(col)\n\nstacked = mp.drop_unnecessary_cols(stacked)\nstacked2021 = mp.drop_unnecessary_cols(stacked2021,drop_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:31.016115Z","iopub.execute_input":"2021-08-31T01:17:31.016496Z","iopub.status.idle":"2021-08-31T01:17:48.052646Z","shell.execute_reply.started":"2021-08-31T01:17:31.016469Z","shell.execute_reply":"2021-08-31T01:17:48.051411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mp.user_data_copy = mp.user_data.copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:48.053866Z","iopub.execute_input":"2021-08-31T01:17:48.054164Z","iopub.status.idle":"2021-08-31T01:17:48.058839Z","shell.execute_reply.started":"2021-08-31T01:17:48.054133Z","shell.execute_reply":"2021-08-31T01:17:48.057872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's merge notifications_data,  user_data, transaction_data\nmp.user_data=mp.user_data_copy.copy()\nmp.user_data = pd.merge(pd.merge(mp.user_data, stacked, right_index=True, how='left', left_on='Customer_ID'), stacked2021, right_index=True, how='left', left_on='Customer_ID')\ndata = pd.merge(mp.user_data,mp.trx_data,on='Customer_ID')\nmp.user_data = mp.reduce_mem_usage(mp.user_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:48.059928Z","iopub.execute_input":"2021-08-31T01:17:48.060233Z","iopub.status.idle":"2021-08-31T01:17:51.099635Z","shell.execute_reply.started":"2021-08-31T01:17:48.060206Z","shell.execute_reply":"2021-08-31T01:17:51.098588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transaction_amount = mp.trx_data.groupby(['Customer_ID'])['Transaction_Amount_Usd'].agg(['sum','min','max','mean','std'])\ntransaction_amount_2021 = mp.trx_data[mp.trx_data['Year_Month'].astype(int)>=202101].groupby(['Customer_ID'])['Transaction_Amount_Usd'].agg(['sum','min','max','mean','std'])\ntransaction_amount.columns = ['_Transaction_Amount_Usd_'+x for x in transaction_amount.columns]\ntransaction_amount_2021.columns = ['_Transaction_Amount_Usd_2021_'+x for x in transaction_amount_2021.columns]\n\ntransaction_type = mp.trx_data.groupby(['Customer_ID','Transaction_Type','Direction','Transaction_Status'])[['Transaction_Type']].count()\ntransaction_type.columns = ['Count']\ntransaction_type = transaction_type.reset_index().pivot(index='Customer_ID',columns=['Transaction_Type','Direction','Transaction_Status'], values='Count')\ntransaction_type.columns = ['_'+x for x in transaction_type.columns.map('_'.join)]\ntransaction_type = mp.drop_unnecessary_cols(transaction_type)\n\nCardholder = mp.trx_data.groupby(['Customer_ID','Cardholder_Verified'])[['Customer_ID']].count()\nCardholder.columns = ['Count']\nCardholder = Cardholder.reset_index().pivot(index='Customer_ID',columns=['Cardholder_Verified'], values='Count')\nCardholder.columns = ['_Cardholder_'+x for x in Cardholder.columns]\n\nCurrency_Code_First = mp.trx_data.groupby('Customer_ID')[['Currency_Code']].agg('first')\nCurrency_Code_First.columns = ['_'+x+'_first' for x in Currency_Code_First.columns]\nCurrency_Code_Last  = mp.trx_data.groupby('Customer_ID')[['Currency_Code']].agg('last')\nCurrency_Code_Last.columns = ['_'+x+'_last' for x in Currency_Code_Last.columns]\n\nMerchant_Country_First = mp.trx_data.groupby('Customer_ID')[['Merchant_Country']].agg('first')\nMerchant_Country_First.columns = ['_'+x+'_first' for x in Merchant_Country_First.columns]\nMerchant_Country_Last  = mp.trx_data.groupby('Customer_ID')[['Merchant_Country']].agg('last')\nMerchant_Country_Last.columns = ['_'+x+'_last' for x in Merchant_Country_Last.columns]\n\nMerchant_MCC_Code_First  = mp.trx_data.groupby('Customer_ID')[['Merchant_MCC_Code']].agg('first')\nMerchant_MCC_Code_First.columns = ['_'+x+'_first' for x in Merchant_MCC_Code_First.columns]\nMerchant_MCC_Code_Last   = mp.trx_data.groupby('Customer_ID')[['Merchant_MCC_Code']].agg('last')\nMerchant_MCC_Code_Last.columns = ['_'+x+'_last' for x in Merchant_MCC_Code_Last.columns]\n\nmp.user_data = pd.merge(mp.user_data, transaction_amount, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, transaction_amount_2021, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, transaction_type, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, Cardholder, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, Currency_Code_First, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, Currency_Code_Last, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, Merchant_Country_First, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, Merchant_Country_Last, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, Merchant_MCC_Code_First, right_index=True, how='left', left_on='Customer_ID')\nmp.user_data = pd.merge(mp.user_data, Merchant_MCC_Code_Last, right_index=True, how='left', left_on='Customer_ID')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:17:51.100896Z","iopub.execute_input":"2021-08-31T01:17:51.101192Z","iopub.status.idle":"2021-08-31T01:18:45.139159Z","shell.execute_reply.started":"2021-08-31T01:17:51.101163Z","shell.execute_reply":"2021-08-31T01:18:45.138273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mp.user_data = mp.reduce_mem_usage(mp.user_data)\ncol_stats = mp.col_stats(mp.user_data)\nfor col in col_stats[col_stats.IS_NULL!=0].index:\n    if col not in [\"_Currency_Code_first\",\"_Currency_Code_last\",\"_Merchant_Country_first\",\"_Merchant_Country_last\",\"_Merchant_MCC_Code_first\",\"_Merchant_MCC_Code_last\"]:\n        mp.user_data[col] = mp.user_data[col].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:18:45.14011Z","iopub.execute_input":"2021-08-31T01:18:45.140382Z","iopub.status.idle":"2021-08-31T01:18:45.445269Z","shell.execute_reply.started":"2021-08-31T01:18:45.140357Z","shell.execute_reply":"2021-08-31T01:18:45.444311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## <a id=\"3M\"><font color=#D65076>3- Model</font></a><br>\n***","metadata":{"execution":{"iopub.status.busy":"2021-08-30T22:15:48.504409Z","iopub.execute_input":"2021-08-30T22:15:48.504871Z","iopub.status.idle":"2021-08-30T22:15:48.511802Z","shell.execute_reply.started":"2021-08-30T22:15:48.504836Z","shell.execute_reply":"2021-08-30T22:15:48.510243Z"}}},{"cell_type":"markdown","source":"#### &#9654;<a id=\"3CLV\"><font color=#009B77> Customer LifeTime Value (CLV)</font></a>","metadata":{}},{"cell_type":"code","source":"outlier = Outlier(data, 'Transaction_Amount_Usd', 0.01, 0.99)\ndisplay(outlier.grab_outliers()['Transaction_Amount_Usd'].unique())\ndata = outlier.replace_with_thresholds()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:18:45.446456Z","iopub.execute_input":"2021-08-31T01:18:45.446728Z","iopub.status.idle":"2021-08-31T01:18:46.751903Z","shell.execute_reply.started":"2021-08-31T01:18:45.446701Z","shell.execute_reply":"2021-08-31T01:18:46.75102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data.loc[data['Transaction_Status']=='COMPLETED',['Transaction_ID','Customer_ID','Transaction_Date','Transaction_Amount_Usd']]\n\nnow = df.Transaction_Date.max()\nrfm = df.groupby('Customer_ID').agg({'Transaction_Date': [lambda date: (max(now-date.min(),date.max()-date.min())).days,\n                                                          lambda date: (now-date.max()).days],\n                                     'Transaction_ID': lambda num: num.nunique(),\n                                     'Transaction_Amount_Usd': lambda TotalPrice: TotalPrice.sum()})\n\nrfm.columns = rfm.columns.droplevel(0)\nrfm.columns=['tenure','recency','frequency','monetary']\nrfm.monetary = rfm.monetary / rfm.frequency\nrfm.loc[:,'recency_weekly'] = rfm.recency / 7\nrfm.loc[:,'tenure_weekly'] = rfm.tenure / 7\n\nrfm = rfm[(rfm['frequency'] > 1)]\nrfm[rfm.recency>rfm.tenure]","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:18:46.75347Z","iopub.execute_input":"2021-08-31T01:18:46.754196Z","iopub.status.idle":"2021-08-31T01:18:59.168589Z","shell.execute_reply.started":"2021-08-31T01:18:46.75414Z","shell.execute_reply":"2021-08-31T01:18:59.167533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = !pip install lifetimes;\nfrom lifetimes import BetaGeoFitter, GammaGammaFitter\n\nbgf = BetaGeoFitter(penalizer_coef=0.001)\n\nbgf.fit(rfm['frequency'],\n        rfm['recency_weekly'],\n        rfm['tenure_weekly'])\n\nggf = GammaGammaFitter(penalizer_coef=0.01)\nggf.fit(rfm['frequency'], rfm['monetary']);\n\ncltv = ggf.customer_lifetime_value(bgf,\n                                   rfm['frequency'],\n                                   rfm['recency_weekly'],\n                                   rfm['tenure_weekly'],\n                                   rfm['monetary'],\n                                   time=3,\n                                   freq=\"W\",\n                                   discount_rate=0.01)\n\npd.set_option('display.float_format', lambda x: '%.5f' % x)\ncltv = pd.DataFrame(cltv).sort_values('clv',ascending=False)\n\ncltv[cltv.clv==0].count()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:38.064595Z","iopub.execute_input":"2021-08-31T01:24:38.065428Z","iopub.status.idle":"2021-08-31T01:24:49.481028Z","shell.execute_reply.started":"2021-08-31T01:24:38.065384Z","shell.execute_reply":"2021-08-31T01:24:49.480143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cltv['Churn']= np.where(cltv.clv==0,1,0)\ncltvrfm = pd.merge(cltv,rfm,left_index=True,right_on='Customer_ID')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:49.483006Z","iopub.execute_input":"2021-08-31T01:24:49.483458Z","iopub.status.idle":"2021-08-31T01:24:49.496946Z","shell.execute_reply.started":"2021-08-31T01:24:49.483413Z","shell.execute_reply":"2021-08-31T01:24:49.49598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cltvrfm = cltvrfm[['Churn']].reset_index()\ncltvrfm.to_csv('clv.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:49.499211Z","iopub.execute_input":"2021-08-31T01:24:49.499661Z","iopub.status.idle":"2021-08-31T01:24:49.556101Z","shell.execute_reply.started":"2021-08-31T01:24:49.49962Z","shell.execute_reply":"2021-08-31T01:24:49.555331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### &#9654;<a id=\"3L\"><font color=#009B77> LightGBM</font></a>","metadata":{}},{"cell_type":"code","source":"data = mp.user_data.copy()\ntarget_col = 'Is_Engaged'","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:49.55755Z","iopub.execute_input":"2021-08-31T01:24:49.558073Z","iopub.status.idle":"2021-08-31T01:24:49.564462Z","shell.execute_reply.started":"2021-08-31T01:24:49.558026Z","shell.execute_reply":"2021-08-31T01:24:49.56361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_cols, date_cols, num_cols, cat_cols, cat_but_cr_cols, num_but_cat_cols, num_but_cat_but_car_cols = mp.df_cols(data, display_=True, cat_th=32, car_th=10, id_th=1000)\n\n\"\"\"\nengaged_id_array = mp.trx_data[(mp.trx_data.Transaction_Status=='COMPLETED') & (mp.trx_data.Year_Month.astype(int)>=202101)]\\\n                    .groupby(['Customer_ID','Year_Month'])[['Transaction_ID']].count().reset_index().Customer_ID.unique()\n\nmp.user_data['Is_Engaged'] = np.where(mp.user_data.Customer_ID.isin(engaged_id_array),1,0)\n\"\"\";\ndata = pd.merge(data,cltvrfm, on='Customer_ID',how='left')\ndata['Is_Engaged'] = 1-data['Churn']\ndata.drop('Churn',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:49.565821Z","iopub.execute_input":"2021-08-31T01:24:49.566194Z","iopub.status.idle":"2021-08-31T01:24:49.801678Z","shell.execute_reply.started":"2021-08-31T01:24:49.566167Z","shell.execute_reply":"2021-08-31T01:24:49.797141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import *\nimport lightgbm as lgb\nfrom category_encoders.hashing import HashingEncoder","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:49.803628Z","iopub.execute_input":"2021-08-31T01:24:49.804045Z","iopub.status.idle":"2021-08-31T01:24:51.249526Z","shell.execute_reply.started":"2021-08-31T01:24:49.804Z","shell.execute_reply":"2021-08-31T01:24:51.248532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols,\n                               drop_first=drop_first)\n    return dataframe\n\ndef rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() /\n                        len(temp_df) < rare_perc).any(axis=None)]\n    print(rare_columns)\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() / len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare',\n                                temp_df[var])\n    return temp_df\n\ndef drop_rare_elements(dataframe, column_name='', drop_text='Others', threshold=0.05):\n    total = dataframe[column_name].shape[0]\n    percentage = dataframe[column_name].value_counts() / total\n    rare_elements_list = []\n    for k,v in percentage.items():\n        if v < threshold:\n            rare_elements_list.append(k)\n    dataframe.loc[dataframe[column_name].isin(rare_elements_list),column_name] = drop_text\n    return dataframe\n\ndata = one_hot_encoder(data,[x for x in cat_cols+num_but_cat_cols if x != target_col],drop_first=True)\ndata = mp.drop_unnecessary_cols(data)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:51.250555Z","iopub.execute_input":"2021-08-31T01:24:51.25083Z","iopub.status.idle":"2021-08-31T01:24:51.484968Z","shell.execute_reply.started":"2021-08-31T01:24:51.250804Z","shell.execute_reply":"2021-08-31T01:24:51.483959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_na = data[data[target_col].isna()]\ndata = data[data[target_col].notna()]\n\nX = data.drop([x for x in data.columns if (x in ['Customer_ID',target_col])],axis=1)\ny = data[target_col].astype('int')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:51.486981Z","iopub.execute_input":"2021-08-31T01:24:51.487296Z","iopub.status.idle":"2021-08-31T01:24:51.546781Z","shell.execute_reply.started":"2021-08-31T01:24:51.487266Z","shell.execute_reply":"2021-08-31T01:24:51.545749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params={\n         'learning_rate': 0.014,\n         'max_depth': 8,\n         'n_estimators': 2000\n}\n\nclass_weights = dict(zip([0,1],(y_train.value_counts()/y_train.shape[0]*5).values))\n\nlgbm_model = lgb.LGBMClassifier(**params,is_unbalance=True,class_weight=class_weights).fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\ny_pred = lgbm_model.predict(X_test)                                                                                  \n\n\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('LightGBM Model balanced accuracy score: {0:0.4f}'.format(balanced_accuracy_score(y_test, y_pred)))\nprint('LightGBM Model precision score: {0:0.4f}'.format(precision_score(y_test, y_pred)))\nprint('LightGBM Model recall score: {0:0.4f}'.format(recall_score(y_test, y_pred)))\nprint('LightGBM Model f1 score: {0:0.4f}'.format(f1_score(y_test, y_pred)))\nprint('LightGBM Model fbeta score: {0:0.4f}'.format(fbeta_score(y_test, y_pred, 0.5)))\nprint('LightGBM Model roc auc score: {0:0.4f}'.format(roc_auc_score(y_test, y_pred)))\nprint('LightGBM Model cohen kappa score: {0:0.4f}'.format(cohen_kappa_score(y_test, y_pred)))\nprint('LightGBM Model matthews correlation coefficient score: {0:0.4f}'.format(matthews_corrcoef(y_test, y_pred)))\n\ncm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = cm.ravel()\nprint(cm)\nprint('LightGBM Model false positive rate: {0:0.4f}'.format(fp / (fp + tn)))\nprint('LightGBM Model false negative rate: {0:0.4f}'.format( fn / (tp + fn)))","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:24:51.548581Z","iopub.execute_input":"2021-08-31T01:24:51.5489Z","iopub.status.idle":"2021-08-31T01:25:01.508928Z","shell.execute_reply.started":"2021-08-31T01:24:51.548871Z","shell.execute_reply":"2021-08-31T01:25:01.507606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotImp(model, X , num = 20, fig_size = (20, 10)):\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    plt.figure(figsize=fig_size)\n    sns.set(font_scale = 2)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances-01.png')\n    plt.show()\n    \nplotImp(lgbm_model, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T01:25:01.51064Z","iopub.execute_input":"2021-08-31T01:25:01.511051Z","iopub.status.idle":"2021-08-31T01:25:02.212514Z","shell.execute_reply.started":"2021-08-31T01:25:01.511008Z","shell.execute_reply":"2021-08-31T01:25:02.211544Z"},"trusted":true},"execution_count":null,"outputs":[]}]}